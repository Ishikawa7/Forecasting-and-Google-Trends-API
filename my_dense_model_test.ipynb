{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Ospiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-03</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-04</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-05</td>\n",
       "      <td>571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Data  Ospiti\n",
       "0    2021-06-01   319.0\n",
       "1    2021-06-02   396.0\n",
       "2    2021-06-03   188.0\n",
       "3    2021-06-04   258.0\n",
       "4    2021-06-05   571.0\n",
       "..          ...     ...\n",
       "542  2022-11-26   732.0\n",
       "543  2022-11-27   495.0\n",
       "544  2022-11-28   226.0\n",
       "545  2022-11-29   289.0\n",
       "546  2022-11-30   247.0\n",
       "\n",
       "[547 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "y = pd.read_csv(\"input/Ospiti.csv\")\n",
    "y[\"Data\"] = pd.to_datetime(y[\"Data\"]).dt.date\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18949343, 0.26172608, 0.06660413, 0.13227017, 0.42589118,\n",
       "        0.34803002, 0.09099437, 0.13320826, 0.24015009, 0.21294559,\n",
       "        0.20262664, 0.33395872, 0.22045028, 0.09662289, 0.20731707,\n",
       "        0.0750469 , 0.14446529, 0.21857411, 0.35928705, 0.21388368,\n",
       "        0.09380863, 0.14446529, 0.20356473, 0.2054409 , 0.22045028,\n",
       "        0.14821764, 0.19512195, 0.15572233, 0.18761726, 0.1641651 ,\n",
       "        0.1575985 , 0.09474672, 0.34990619, 0.29268293, 0.17354597,\n",
       "        0.07879925, 0.21763602, 0.26547842, 0.22326454, 0.33020638,\n",
       "        0.06378987, 0.0891182 , 0.17260788, 0.16979362, 0.21669794,\n",
       "        0.18667917, 0.43621013, 0.17729831, 0.19699812, 0.16510319,\n",
       "        0.20450281, 0.20168856, 0.17917448, 0.33020638, 0.29831144,\n",
       "        0.15478424, 0.17448405, 0.21482176, 0.18105066, 0.22795497,\n",
       "        0.39118199, 0.24859287, 0.22326454, 0.18761726, 0.28611632,\n",
       "        0.27861163, 0.16510319, 0.22514071, 0.14540338, 0.12382739,\n",
       "        0.14071295, 0.12007505, 0.2532833 , 0.16604128, 0.19512195,\n",
       "        0.13414634, 0.20825516, 0.25046904, 0.24484053, 0.24108818,\n",
       "        0.28424015, 0.3836773 , 0.25140713, 0.05347092, 0.13320826,\n",
       "        0.16228893, 0.1641651 , 0.17542214, 0.37054409, 0.25422139,\n",
       "        0.12476548, 0.08630394, 0.15947467, 0.17542214, 0.2260788 ,\n",
       "        0.4380863 , 0.31707317, 0.16510319, 0.13227017, 0.20168856,\n",
       "        0.22420263, 0.24390244, 0.46405613, 0.22045028, 0.16041276,\n",
       "        0.12476548, 0.17448405, 0.20919325, 0.26641651, 0.58442777,\n",
       "        0.32926829, 0.11913696, 0.18386492, 0.22138837, 0.19606004,\n",
       "        0.24202627, 0.53377111, 0.23733583, 0.11444653, 0.16135084,\n",
       "        0.16885553, 0.24296435, 0.30300188, 0.5891182 , 0.2739212 ,\n",
       "        0.08818011, 0.14446529, 0.16979362, 0.18292683, 0.25703565,\n",
       "        0.64258912, 0.3902439 , 0.10037523, 0.18386492, 0.18198874,\n",
       "        0.17354597, 0.3564728 , 0.51219512, 0.36210131, 0.15103189,\n",
       "        0.1435272 , 0.184803  , 0.22420263, 0.22889306, 0.56003752,\n",
       "        0.32926829, 0.11726079, 0.14727955, 0.15103189, 0.20919325,\n",
       "        0.28705441, 0.53658537, 0.45590994, 0.3564728 , 0.12851782,\n",
       "        0.184803  , 0.16791745, 0.27485929, 0.56285178, 0.27204503,\n",
       "        0.13602251, 0.15853659, 0.11257036, 0.15666041, 0.27298311,\n",
       "        0.70168856, 0.24953096, 0.13414634, 0.21294559, 0.12945591,\n",
       "        0.18761726, 0.28986867, 0.49624765, 0.31988743, 0.14165103,\n",
       "        0.14915572, 0.18667917, 0.16510319, 0.25609756, 0.53283302,\n",
       "        0.26266417, 0.13508443, 0.19418386, 0.15009381, 0.18292683,\n",
       "        0.33020638, 0.49812383, 0.25797373, 0.10694184, 0.2120075 ,\n",
       "        0.10131332, 0.13508443, 0.30487805, 0.42964353, 0.19606004,\n",
       "        0.13789869, 0.26641651, 0.23170732, 0.24765478, 0.3902439 ,\n",
       "        0.46998124, 0.34709193, 0.23545966, 0.25797373, 0.28236398,\n",
       "        0.25797373, 0.11913696, 0.06472795, 0.06941839, 0.12664165,\n",
       "        0.0891182 , 0.08536585, 0.15853659, 0.10412758, 0.21763602,\n",
       "        0.07692308, 0.1097561 , 0.13977486, 0.12757974, 0.09756098,\n",
       "        0.20450281, 0.09380863, 0.00750469, 0.        , 0.2532833 ,\n",
       "        0.03658537, 0.07692308, 0.25703565, 0.12288931, 0.04502814,\n",
       "        0.08442777, 0.10787992, 0.11350844, 0.11726079, 0.35834897,\n",
       "        0.16041276, 0.0412758 , 0.09005629, 0.11163227, 0.09287054,\n",
       "        0.21294559, 0.41744841, 0.19606004, 0.0478424 , 0.12288931,\n",
       "        0.17073171, 0.10131332, 0.22326454, 0.44465291, 0.23076923,\n",
       "        0.07223265, 0.08818011, 0.09380863, 0.14165103, 0.20825516,\n",
       "        1.        , 0.34427767, 0.22326454, 0.16228893, 0.11913696,\n",
       "        0.1641651 , 0.28986867, 0.59756098, 0.369606  , 0.09380863,\n",
       "        0.18386492, 0.17542214, 0.12664165, 0.28517824, 0.65947467,\n",
       "        0.31613508, 0.10694184, 0.16697936, 0.20825516, 0.19512195,\n",
       "        0.25609756, 0.54596623, 0.3424015 , 0.09756098, 0.18574109,\n",
       "        0.20075047, 0.20168856, 0.18667917, 0.48592871, 0.34896811,\n",
       "        0.07410882, 0.16791745, 0.14915572, 0.19043152, 0.2879925 ,\n",
       "        0.69699812, 0.42964353, 0.12476548, 0.15103189, 0.15572233,\n",
       "        0.16979362, 0.25984991, 0.50750469, 0.29268293, 0.11913696,\n",
       "        0.09474672, 0.17073171, 0.12570356, 0.24390244, 0.7054409 ,\n",
       "        0.35272045, 0.13508443, 0.1988743 , 0.18105066, 0.19043152,\n",
       "        0.28517824, 0.59005629, 0.30300188, 0.16510319, 0.21763602,\n",
       "        0.15290807, 0.23639775, 0.22983114, 0.2532833 , 0.36772983,\n",
       "        0.24859287, 0.07223265, 0.13977486, 0.16979362, 0.25140713,\n",
       "        0.43902439, 0.40056285, 0.34427767, 0.12101313, 0.17542214,\n",
       "        0.16979362, 0.20825516, 0.61726079, 0.38742964, 0.09849906,\n",
       "        0.13883677, 0.15947467, 0.20919325, 0.27767355, 0.68198874,\n",
       "        0.80863039, 0.18292683, 0.14634146, 0.14727955, 0.24390244,\n",
       "        0.24577861, 0.53189493, 0.41369606, 0.1435272 , 0.1988743 ,\n",
       "        0.18105066, 0.18386492, 0.26266417, 0.52157598, 0.30018762,\n",
       "        0.1575985 , 0.14915572, 0.16322702, 0.25140713, 0.34521576,\n",
       "        0.49624765, 0.43339587, 0.12945591, 0.1988743 , 0.20825516,\n",
       "        0.41275797, 0.22514071, 0.38273921, 0.32082552, 0.15478424,\n",
       "        0.2260788 , 0.29831144, 0.18761726, 0.29362101, 0.37992495,\n",
       "        0.23076923, 0.12945591, 0.23452158, 0.21763602, 0.21294559,\n",
       "        0.29643527, 0.45778612, 0.25891182, 0.13227017, 0.21482176,\n",
       "        0.26547842, 0.2054409 , 0.33302064, 0.3902439 , 0.23827392,\n",
       "        0.15196998, 0.23639775, 0.22795497, 0.23545966, 0.23827392,\n",
       "        0.43902439, 0.26547842, 0.1369606 , 0.17260788, 0.2326454 ,\n",
       "        0.24390244, 0.22983114, 0.40150094, 0.25609756, 0.17260788,\n",
       "        0.20731707, 0.21763602, 0.17917448, 0.26923077, 0.34803002,\n",
       "        0.28611632, 0.15103189, 0.19418386, 0.23639775, 0.2120075 ,\n",
       "        0.18574109, 0.44465291, 0.28236398, 0.2195122 , 0.23921201,\n",
       "        0.2467167 , 0.2532833 , 0.24902638, 0.41181989, 0.25797373,\n",
       "        0.19699812, 0.22514071, 0.2195122 , 0.28893058, 0.29924953,\n",
       "        0.44090056, 0.37992495, 0.22514071, 0.18198874, 0.26266417,\n",
       "        0.21857411, 0.31425891, 0.31238274, 0.28142589, 0.63133208,\n",
       "        0.29080675, 0.25703565, 0.36679174, 0.42120075, 0.48592871,\n",
       "        0.38649156, 0.22514071, 0.25140713, 0.23358349, 0.28142589,\n",
       "        0.21575985, 0.48592871, 0.41181989, 0.15478424, 0.13320826,\n",
       "        0.2195122 , 0.20450281, 0.33771107, 0.52814259, 0.30018762,\n",
       "        0.20168856, 0.16979362, 0.22795497, 0.2467167 , 0.34896811,\n",
       "        0.48030019, 0.35834897, 0.1782364 , 0.2195122 , 0.184803  ,\n",
       "        0.14821764, 0.33771107, 0.62007505, 0.2673546 , 0.14540338,\n",
       "        0.22138837, 0.22514071, 0.15009381, 0.3011257 , 0.61350844,\n",
       "        0.35928705, 0.11257036, 0.13977486, 0.25422139, 0.16228893,\n",
       "        0.23639775, 0.63414634, 0.42589118, 0.15384615, 0.12195122,\n",
       "        0.20825516, 0.19981238, 0.26454034, 0.56097561, 0.42870544,\n",
       "        0.15947467, 0.13133208, 0.12945591, 0.16041276, 0.26923077,\n",
       "        0.68292683, 0.3424015 , 0.09662289, 0.17729831, 0.2054409 ,\n",
       "        0.22889306, 0.26547842, 0.7467167 , 0.43621013, 0.13320826,\n",
       "        0.1782364 , 0.17917448, 0.28893058, 0.29174484, 0.47185741,\n",
       "        0.48217636, 0.43433396, 0.34896811, 0.09756098, 0.1575985 ,\n",
       "        0.23827392, 0.5478424 , 0.29831144, 0.08348968, 0.16604128,\n",
       "        0.21763602, 0.17542214, 0.28330206, 0.65384615, 0.2945591 ,\n",
       "        0.07223265, 0.15666041, 0.19230769, 0.17729831, 0.33020638,\n",
       "        0.54409006, 0.39868668, 0.12382739, 0.19418386, 0.18574109,\n",
       "        0.18574109, 0.24202627, 0.57692308, 0.35459662, 0.10225141,\n",
       "        0.16135084, 0.12195122]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y[\"Ospiti\"].values.reshape(-1,1))\n",
    "y_scaled = np.reshape(y_scaled, (1, len(y_scaled)))\n",
    "y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 15:52:20.501645: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-16 15:52:20.501698: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (davide-Inspiron-3891): /proc/driver/nvidia/version does not exist\n",
      "2022-12-16 15:52:20.502709: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf_dataset = tf.data.Dataset.from_tensor_slices(y_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
    "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "\n",
    "  def sub_to_batch(sub):\n",
    "    return sub.batch(window_size, drop_remainder=True)\n",
    "\n",
    "  windows = windows.flat_map(sub_to_batch)\n",
    "  return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18949343 0.26172608 0.06660413 0.13227017 0.42589118 0.34803002\n",
      " 0.09099437 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872\n",
      " 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411\n",
      " 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409\n",
      " 0.22045028 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651\n",
      " 0.1575985  0.09474672 0.34990619 0.29268293 0.17354597 0.07879925\n",
      " 0.21763602]\n",
      "[0.26172608 0.06660413 0.13227017 0.42589118 0.34803002 0.09099437\n",
      " 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028\n",
      " 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705\n",
      " 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028\n",
      " 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985\n",
      " 0.09474672 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602\n",
      " 0.26547842]\n",
      "[0.06660413 0.13227017 0.42589118 0.34803002 0.09099437 0.13320826\n",
      " 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289\n",
      " 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368\n",
      " 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764\n",
      " 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672\n",
      " 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842\n",
      " 0.22326454]\n",
      "[0.13227017 0.42589118 0.34803002 0.09099437 0.13320826 0.24015009\n",
      " 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289 0.20731707\n",
      " 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368 0.09380863\n",
      " 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764 0.19512195\n",
      " 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672 0.34990619\n",
      " 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842 0.22326454\n",
      " 0.33020638]\n",
      "[0.42589118 0.34803002 0.09099437 0.13320826 0.24015009 0.21294559\n",
      " 0.20262664 0.33395872 0.22045028 0.09662289 0.20731707 0.0750469\n",
      " 0.14446529 0.21857411 0.35928705 0.21388368 0.09380863 0.14446529\n",
      " 0.20356473 0.2054409  0.22045028 0.14821764 0.19512195 0.15572233\n",
      " 0.18761726 0.1641651  0.1575985  0.09474672 0.34990619 0.29268293\n",
      " 0.17354597 0.07879925 0.21763602 0.26547842 0.22326454 0.33020638\n",
      " 0.06378987]\n",
      "[0.34803002 0.09099437 0.13320826 0.24015009 0.21294559 0.20262664\n",
      " 0.33395872 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529\n",
      " 0.21857411 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473\n",
      " 0.2054409  0.22045028 0.14821764 0.19512195 0.15572233 0.18761726\n",
      " 0.1641651  0.1575985  0.09474672 0.34990619 0.29268293 0.17354597\n",
      " 0.07879925 0.21763602 0.26547842 0.22326454 0.33020638 0.06378987\n",
      " 0.0891182 ]\n",
      "[0.09099437 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872\n",
      " 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411\n",
      " 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409\n",
      " 0.22045028 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651\n",
      " 0.1575985  0.09474672 0.34990619 0.29268293 0.17354597 0.07879925\n",
      " 0.21763602 0.26547842 0.22326454 0.33020638 0.06378987 0.0891182\n",
      " 0.17260788]\n",
      "[0.13320826 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028\n",
      " 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705\n",
      " 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028\n",
      " 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985\n",
      " 0.09474672 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602\n",
      " 0.26547842 0.22326454 0.33020638 0.06378987 0.0891182  0.17260788\n",
      " 0.16979362]\n",
      "[0.24015009 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289\n",
      " 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368\n",
      " 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764\n",
      " 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672\n",
      " 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842\n",
      " 0.22326454 0.33020638 0.06378987 0.0891182  0.17260788 0.16979362\n",
      " 0.21669794]\n",
      "[0.21294559 0.20262664 0.33395872 0.22045028 0.09662289 0.20731707\n",
      " 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368 0.09380863\n",
      " 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764 0.19512195\n",
      " 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672 0.34990619\n",
      " 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842 0.22326454\n",
      " 0.33020638 0.06378987 0.0891182  0.17260788 0.16979362 0.21669794\n",
      " 0.18667917]\n"
     ]
    }
   ],
   "source": [
    "ds = make_window_dataset(tf_dataset, window_size=37, shift = 1, stride=1)\n",
    "\n",
    "for example in ds.take(10):\n",
    "  print(example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_7_step(batch):\n",
    "  # Shift features and labels one step relative to each other.\n",
    "  return batch[:-7], batch[-7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18949343 0.26172608 0.06660413 0.13227017 0.42589118 0.34803002\n",
      " 0.09099437 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872\n",
      " 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411\n",
      " 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409\n",
      " 0.22045028 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651 ] => [0.1575985  0.09474672 0.34990619 0.29268293 0.17354597 0.07879925\n",
      " 0.21763602]\n",
      "[0.26172608 0.06660413 0.13227017 0.42589118 0.34803002 0.09099437\n",
      " 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028\n",
      " 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705\n",
      " 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028\n",
      " 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985 ] => [0.09474672 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602\n",
      " 0.26547842]\n",
      "[0.06660413 0.13227017 0.42589118 0.34803002 0.09099437 0.13320826\n",
      " 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289\n",
      " 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368\n",
      " 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764\n",
      " 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672] => [0.34990619 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842\n",
      " 0.22326454]\n"
     ]
    }
   ],
   "source": [
    "dense_labels_ds = ds.map(dense_7_step)\n",
    "\n",
    "#create a tensor for holding the labels and features\n",
    "all_inputs = []\n",
    "all_labels = []\n",
    "\n",
    "for inputs,labels in dense_labels_ds.take(3):\n",
    "  print(inputs.numpy(), \"=>\", labels.numpy())\n",
    "\n",
    "for inputs,labels in dense_labels_ds:\n",
    "  all_inputs.append(inputs)\n",
    "  all_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(511, 30), dtype=float64, numpy=\n",
       "array([[0.18949343, 0.26172608, 0.06660413, ..., 0.15572233, 0.18761726,\n",
       "        0.1641651 ],\n",
       "       [0.26172608, 0.06660413, 0.13227017, ..., 0.18761726, 0.1641651 ,\n",
       "        0.1575985 ],\n",
       "       [0.06660413, 0.13227017, 0.42589118, ..., 0.1641651 , 0.1575985 ,\n",
       "        0.09474672],\n",
       "       ...,\n",
       "       [0.43621013, 0.13320826, 0.1782364 , ..., 0.54409006, 0.39868668,\n",
       "        0.12382739],\n",
       "       [0.13320826, 0.1782364 , 0.17917448, ..., 0.39868668, 0.12382739,\n",
       "        0.19418386],\n",
       "       [0.1782364 , 0.17917448, 0.28893058, ..., 0.12382739, 0.19418386,\n",
       "        0.18574109]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.concat(all_inputs, axis=0)\n",
    "X = tf.reshape(X, (-1, 30))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(511, 7), dtype=float64, numpy=\n",
       "array([[0.1575985 , 0.09474672, 0.34990619, ..., 0.17354597, 0.07879925,\n",
       "        0.21763602],\n",
       "       [0.09474672, 0.34990619, 0.29268293, ..., 0.07879925, 0.21763602,\n",
       "        0.26547842],\n",
       "       [0.34990619, 0.29268293, 0.17354597, ..., 0.21763602, 0.26547842,\n",
       "        0.22326454],\n",
       "       ...,\n",
       "       [0.19418386, 0.18574109, 0.18574109, ..., 0.57692308, 0.35459662,\n",
       "        0.10225141],\n",
       "       [0.18574109, 0.18574109, 0.24202627, ..., 0.35459662, 0.10225141,\n",
       "        0.16135084],\n",
       "       [0.18574109, 0.24202627, 0.57692308, ..., 0.10225141, 0.16135084,\n",
       "        0.12195122]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = tf.concat(all_labels, axis=0)\n",
    "Y = tf.reshape(Y, (-1, 7))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a forecasting model using the Keras Sequential API\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(256, activation=\"relu\", name=\"layer1\"),\n",
    "        # add dropout to prevent overfitting\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation=\"relu\", name=\"layer3\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(7, name=\"layer4\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 1s 2ms/step - loss: 0.0470\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0113\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0114\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0107\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0105\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0100\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0088\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0088\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0084\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0086\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0083\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0080\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0085\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0082\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0074\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0073\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0072\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0069\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0068\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0067\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0067\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0062\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0057\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0056\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0051\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0050\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0051\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0047\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0045\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0046\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0047\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0042\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0041\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0041\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0040\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0036\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0036\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0035\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0032\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0033\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0034\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0032\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0033\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0032\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0031\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0028\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0028\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0030\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0030\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0029\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0028\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0028\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0028\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0027\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0027\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0026\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0028\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0027\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0027\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0027\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0023\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0025\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0025\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0026\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0024\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0023\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0025\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0022\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0024\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0022\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0023\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0023\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0024\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0021\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0022\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0023\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0022\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0021\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0023\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0022\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabd44eaa90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=Y ,epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a request for forecasting the next 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Ospiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>2022-11-06</td>\n",
       "      <td>435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>2022-11-07</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>2022-11-09</td>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2022-11-11</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2022-11-12</td>\n",
       "      <td>814.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>2022-11-20</td>\n",
       "      <td>542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>2022-11-22</td>\n",
       "      <td>324.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2022-11-24</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Data  Ospiti\n",
       "517  2022-11-01   489.0\n",
       "518  2022-11-02   221.0\n",
       "519  2022-11-03   285.0\n",
       "520  2022-11-04   371.0\n",
       "521  2022-11-05   701.0\n",
       "522  2022-11-06   435.0\n",
       "523  2022-11-07   206.0\n",
       "524  2022-11-08   294.0\n",
       "525  2022-11-09   349.0\n",
       "526  2022-11-10   304.0\n",
       "527  2022-11-11   419.0\n",
       "528  2022-11-12   814.0\n",
       "529  2022-11-13   431.0\n",
       "530  2022-11-14   194.0\n",
       "531  2022-11-15   284.0\n",
       "532  2022-11-16   322.0\n",
       "533  2022-11-17   306.0\n",
       "534  2022-11-18   469.0\n",
       "535  2022-11-19   697.0\n",
       "536  2022-11-20   542.0\n",
       "537  2022-11-21   249.0\n",
       "538  2022-11-22   324.0\n",
       "539  2022-11-23   315.0\n",
       "540  2022-11-24   315.0\n",
       "541  2022-11-25   375.0\n",
       "542  2022-11-26   732.0\n",
       "543  2022-11-27   495.0\n",
       "544  2022-11-28   226.0\n",
       "545  2022-11-29   289.0\n",
       "546  2022-11-30   247.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y.iloc[-30:]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47580645, 0.04354839, 0.14677419, 0.28548387, 0.81774194,\n",
       "        0.38870968, 0.01935484, 0.16129032, 0.25      , 0.17741935,\n",
       "        0.36290323, 1.        , 0.38225806, 0.        , 0.14516129,\n",
       "        0.20645161, 0.18064516, 0.44354839, 0.81129032, 0.56129032,\n",
       "        0.08870968, 0.20967742, 0.19516129, 0.19516129, 0.29193548,\n",
       "        0.86774194, 0.48548387, 0.0516129 , 0.15322581, 0.08548387]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_scaled = scaler.fit_transform(y_test[\"Ospiti\"].values.reshape(-1,1))\n",
    "y_test_scaled = np.reshape(y_test_scaled, (1, len(y_test_scaled)))\n",
    "y_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-12-01', '2022-12-02', '2022-12-03', '2022-12-04',\n",
       "               '2022-12-05', '2022-12-06', '2022-12-07'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dates to be predicted\n",
    "dates = pd.date_range(start=y[\"Data\"].iloc[-1],periods=8)[1:]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19687265, 0.31422597, 0.7861344 , 0.40817857, 0.10498112,\n",
       "        0.17009383, 0.20105983]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the next 7 days\n",
    "pred = model.predict(y_test_scaled)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[316.06104, 388.8201 , 681.4033 , 447.0707 , 259.0883 , 299.4582 ,\n",
       "        318.65707]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_unscaled = scaler.inverse_transform(pred)\n",
    "pred_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ospiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>316.061035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>388.820099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-03</th>\n",
       "      <td>681.403320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-04</th>\n",
       "      <td>447.070709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>259.088287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>299.458191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>318.657074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Ospiti\n",
       "2022-12-01  316.061035\n",
       "2022-12-02  388.820099\n",
       "2022-12-03  681.403320\n",
       "2022-12-04  447.070709\n",
       "2022-12-05  259.088287\n",
       "2022-12-06  299.458191\n",
       "2022-12-07  318.657074"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct results dataframe\n",
    "results = pd.DataFrame(pred_unscaled[0], columns=[\"Ospiti\"], index=dates)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 15:52:44.290360: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c7637a5d-34e6-4c3d-9968-23def59bfaf5/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model with pickle\n",
    "import pickle\n",
    "pickle.dump(model, open(\"models/my_dense_model\", 'wb'))\n",
    "\n",
    "# save the scaler with pickle\n",
    "pickle.dump(scaler, open(\"models/scaler\", 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
