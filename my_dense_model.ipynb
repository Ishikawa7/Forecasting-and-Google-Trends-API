{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Ospiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-01</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-03</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-04</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-05</td>\n",
       "      <td>571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Data  Ospiti\n",
       "0    2021-06-01   319.0\n",
       "1    2021-06-02   396.0\n",
       "2    2021-06-03   188.0\n",
       "3    2021-06-04   258.0\n",
       "4    2021-06-05   571.0\n",
       "..          ...     ...\n",
       "542  2022-11-26   732.0\n",
       "543  2022-11-27   495.0\n",
       "544  2022-11-28   226.0\n",
       "545  2022-11-29   289.0\n",
       "546  2022-11-30   247.0\n",
       "\n",
       "[547 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "y = pd.read_csv(\"input/Ospiti.csv\")\n",
    "y[\"Data\"] = pd.to_datetime(y[\"Data\"]).dt.date\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18949343, 0.26172608, 0.06660413, 0.13227017, 0.42589118,\n",
       "        0.34803002, 0.09099437, 0.13320826, 0.24015009, 0.21294559,\n",
       "        0.20262664, 0.33395872, 0.22045028, 0.09662289, 0.20731707,\n",
       "        0.0750469 , 0.14446529, 0.21857411, 0.35928705, 0.21388368,\n",
       "        0.09380863, 0.14446529, 0.20356473, 0.2054409 , 0.22045028,\n",
       "        0.14821764, 0.19512195, 0.15572233, 0.18761726, 0.1641651 ,\n",
       "        0.1575985 , 0.09474672, 0.34990619, 0.29268293, 0.17354597,\n",
       "        0.07879925, 0.21763602, 0.26547842, 0.22326454, 0.33020638,\n",
       "        0.06378987, 0.0891182 , 0.17260788, 0.16979362, 0.21669794,\n",
       "        0.18667917, 0.43621013, 0.17729831, 0.19699812, 0.16510319,\n",
       "        0.20450281, 0.20168856, 0.17917448, 0.33020638, 0.29831144,\n",
       "        0.15478424, 0.17448405, 0.21482176, 0.18105066, 0.22795497,\n",
       "        0.39118199, 0.24859287, 0.22326454, 0.18761726, 0.28611632,\n",
       "        0.27861163, 0.16510319, 0.22514071, 0.14540338, 0.12382739,\n",
       "        0.14071295, 0.12007505, 0.2532833 , 0.16604128, 0.19512195,\n",
       "        0.13414634, 0.20825516, 0.25046904, 0.24484053, 0.24108818,\n",
       "        0.28424015, 0.3836773 , 0.25140713, 0.05347092, 0.13320826,\n",
       "        0.16228893, 0.1641651 , 0.17542214, 0.37054409, 0.25422139,\n",
       "        0.12476548, 0.08630394, 0.15947467, 0.17542214, 0.2260788 ,\n",
       "        0.4380863 , 0.31707317, 0.16510319, 0.13227017, 0.20168856,\n",
       "        0.22420263, 0.24390244, 0.46405613, 0.22045028, 0.16041276,\n",
       "        0.12476548, 0.17448405, 0.20919325, 0.26641651, 0.58442777,\n",
       "        0.32926829, 0.11913696, 0.18386492, 0.22138837, 0.19606004,\n",
       "        0.24202627, 0.53377111, 0.23733583, 0.11444653, 0.16135084,\n",
       "        0.16885553, 0.24296435, 0.30300188, 0.5891182 , 0.2739212 ,\n",
       "        0.08818011, 0.14446529, 0.16979362, 0.18292683, 0.25703565,\n",
       "        0.64258912, 0.3902439 , 0.10037523, 0.18386492, 0.18198874,\n",
       "        0.17354597, 0.3564728 , 0.51219512, 0.36210131, 0.15103189,\n",
       "        0.1435272 , 0.184803  , 0.22420263, 0.22889306, 0.56003752,\n",
       "        0.32926829, 0.11726079, 0.14727955, 0.15103189, 0.20919325,\n",
       "        0.28705441, 0.53658537, 0.45590994, 0.3564728 , 0.12851782,\n",
       "        0.184803  , 0.16791745, 0.27485929, 0.56285178, 0.27204503,\n",
       "        0.13602251, 0.15853659, 0.11257036, 0.15666041, 0.27298311,\n",
       "        0.70168856, 0.24953096, 0.13414634, 0.21294559, 0.12945591,\n",
       "        0.18761726, 0.28986867, 0.49624765, 0.31988743, 0.14165103,\n",
       "        0.14915572, 0.18667917, 0.16510319, 0.25609756, 0.53283302,\n",
       "        0.26266417, 0.13508443, 0.19418386, 0.15009381, 0.18292683,\n",
       "        0.33020638, 0.49812383, 0.25797373, 0.10694184, 0.2120075 ,\n",
       "        0.10131332, 0.13508443, 0.30487805, 0.42964353, 0.19606004,\n",
       "        0.13789869, 0.26641651, 0.23170732, 0.24765478, 0.3902439 ,\n",
       "        0.46998124, 0.34709193, 0.23545966, 0.25797373, 0.28236398,\n",
       "        0.25797373, 0.11913696, 0.06472795, 0.06941839, 0.12664165,\n",
       "        0.0891182 , 0.08536585, 0.15853659, 0.10412758, 0.21763602,\n",
       "        0.07692308, 0.1097561 , 0.13977486, 0.12757974, 0.09756098,\n",
       "        0.20450281, 0.09380863, 0.00750469, 0.        , 0.2532833 ,\n",
       "        0.03658537, 0.07692308, 0.25703565, 0.12288931, 0.04502814,\n",
       "        0.08442777, 0.10787992, 0.11350844, 0.11726079, 0.35834897,\n",
       "        0.16041276, 0.0412758 , 0.09005629, 0.11163227, 0.09287054,\n",
       "        0.21294559, 0.41744841, 0.19606004, 0.0478424 , 0.12288931,\n",
       "        0.17073171, 0.10131332, 0.22326454, 0.44465291, 0.23076923,\n",
       "        0.07223265, 0.08818011, 0.09380863, 0.14165103, 0.20825516,\n",
       "        1.        , 0.34427767, 0.22326454, 0.16228893, 0.11913696,\n",
       "        0.1641651 , 0.28986867, 0.59756098, 0.369606  , 0.09380863,\n",
       "        0.18386492, 0.17542214, 0.12664165, 0.28517824, 0.65947467,\n",
       "        0.31613508, 0.10694184, 0.16697936, 0.20825516, 0.19512195,\n",
       "        0.25609756, 0.54596623, 0.3424015 , 0.09756098, 0.18574109,\n",
       "        0.20075047, 0.20168856, 0.18667917, 0.48592871, 0.34896811,\n",
       "        0.07410882, 0.16791745, 0.14915572, 0.19043152, 0.2879925 ,\n",
       "        0.69699812, 0.42964353, 0.12476548, 0.15103189, 0.15572233,\n",
       "        0.16979362, 0.25984991, 0.50750469, 0.29268293, 0.11913696,\n",
       "        0.09474672, 0.17073171, 0.12570356, 0.24390244, 0.7054409 ,\n",
       "        0.35272045, 0.13508443, 0.1988743 , 0.18105066, 0.19043152,\n",
       "        0.28517824, 0.59005629, 0.30300188, 0.16510319, 0.21763602,\n",
       "        0.15290807, 0.23639775, 0.22983114, 0.2532833 , 0.36772983,\n",
       "        0.24859287, 0.07223265, 0.13977486, 0.16979362, 0.25140713,\n",
       "        0.43902439, 0.40056285, 0.34427767, 0.12101313, 0.17542214,\n",
       "        0.16979362, 0.20825516, 0.61726079, 0.38742964, 0.09849906,\n",
       "        0.13883677, 0.15947467, 0.20919325, 0.27767355, 0.68198874,\n",
       "        0.80863039, 0.18292683, 0.14634146, 0.14727955, 0.24390244,\n",
       "        0.24577861, 0.53189493, 0.41369606, 0.1435272 , 0.1988743 ,\n",
       "        0.18105066, 0.18386492, 0.26266417, 0.52157598, 0.30018762,\n",
       "        0.1575985 , 0.14915572, 0.16322702, 0.25140713, 0.34521576,\n",
       "        0.49624765, 0.43339587, 0.12945591, 0.1988743 , 0.20825516,\n",
       "        0.41275797, 0.22514071, 0.38273921, 0.32082552, 0.15478424,\n",
       "        0.2260788 , 0.29831144, 0.18761726, 0.29362101, 0.37992495,\n",
       "        0.23076923, 0.12945591, 0.23452158, 0.21763602, 0.21294559,\n",
       "        0.29643527, 0.45778612, 0.25891182, 0.13227017, 0.21482176,\n",
       "        0.26547842, 0.2054409 , 0.33302064, 0.3902439 , 0.23827392,\n",
       "        0.15196998, 0.23639775, 0.22795497, 0.23545966, 0.23827392,\n",
       "        0.43902439, 0.26547842, 0.1369606 , 0.17260788, 0.2326454 ,\n",
       "        0.24390244, 0.22983114, 0.40150094, 0.25609756, 0.17260788,\n",
       "        0.20731707, 0.21763602, 0.17917448, 0.26923077, 0.34803002,\n",
       "        0.28611632, 0.15103189, 0.19418386, 0.23639775, 0.2120075 ,\n",
       "        0.18574109, 0.44465291, 0.28236398, 0.2195122 , 0.23921201,\n",
       "        0.2467167 , 0.2532833 , 0.24902638, 0.41181989, 0.25797373,\n",
       "        0.19699812, 0.22514071, 0.2195122 , 0.28893058, 0.29924953,\n",
       "        0.44090056, 0.37992495, 0.22514071, 0.18198874, 0.26266417,\n",
       "        0.21857411, 0.31425891, 0.31238274, 0.28142589, 0.63133208,\n",
       "        0.29080675, 0.25703565, 0.36679174, 0.42120075, 0.48592871,\n",
       "        0.38649156, 0.22514071, 0.25140713, 0.23358349, 0.28142589,\n",
       "        0.21575985, 0.48592871, 0.41181989, 0.15478424, 0.13320826,\n",
       "        0.2195122 , 0.20450281, 0.33771107, 0.52814259, 0.30018762,\n",
       "        0.20168856, 0.16979362, 0.22795497, 0.2467167 , 0.34896811,\n",
       "        0.48030019, 0.35834897, 0.1782364 , 0.2195122 , 0.184803  ,\n",
       "        0.14821764, 0.33771107, 0.62007505, 0.2673546 , 0.14540338,\n",
       "        0.22138837, 0.22514071, 0.15009381, 0.3011257 , 0.61350844,\n",
       "        0.35928705, 0.11257036, 0.13977486, 0.25422139, 0.16228893,\n",
       "        0.23639775, 0.63414634, 0.42589118, 0.15384615, 0.12195122,\n",
       "        0.20825516, 0.19981238, 0.26454034, 0.56097561, 0.42870544,\n",
       "        0.15947467, 0.13133208, 0.12945591, 0.16041276, 0.26923077,\n",
       "        0.68292683, 0.3424015 , 0.09662289, 0.17729831, 0.2054409 ,\n",
       "        0.22889306, 0.26547842, 0.7467167 , 0.43621013, 0.13320826,\n",
       "        0.1782364 , 0.17917448, 0.28893058, 0.29174484, 0.47185741,\n",
       "        0.48217636, 0.43433396, 0.34896811, 0.09756098, 0.1575985 ,\n",
       "        0.23827392, 0.5478424 , 0.29831144, 0.08348968, 0.16604128,\n",
       "        0.21763602, 0.17542214, 0.28330206, 0.65384615, 0.2945591 ,\n",
       "        0.07223265, 0.15666041, 0.19230769, 0.17729831, 0.33020638,\n",
       "        0.54409006, 0.39868668, 0.12382739, 0.19418386, 0.18574109,\n",
       "        0.18574109, 0.24202627, 0.57692308, 0.35459662, 0.10225141,\n",
       "        0.16135084, 0.12195122]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y[\"Ospiti\"].values.reshape(-1,1))\n",
    "y_scaled = np.reshape(y_scaled, (1, len(y_scaled)))\n",
    "y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = tf.data.Dataset.from_tensor_slices(y_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
    "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "\n",
    "  def sub_to_batch(sub):\n",
    "    return sub.batch(window_size, drop_remainder=True)\n",
    "\n",
    "  windows = windows.flat_map(sub_to_batch)\n",
    "  return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18949343 0.26172608 0.06660413 0.13227017 0.42589118 0.34803002\n",
      " 0.09099437 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872\n",
      " 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411\n",
      " 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409\n",
      " 0.22045028 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651\n",
      " 0.1575985  0.09474672 0.34990619 0.29268293 0.17354597 0.07879925\n",
      " 0.21763602]\n",
      "[0.26172608 0.06660413 0.13227017 0.42589118 0.34803002 0.09099437\n",
      " 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028\n",
      " 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705\n",
      " 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028\n",
      " 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985\n",
      " 0.09474672 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602\n",
      " 0.26547842]\n",
      "[0.06660413 0.13227017 0.42589118 0.34803002 0.09099437 0.13320826\n",
      " 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289\n",
      " 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368\n",
      " 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764\n",
      " 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672\n",
      " 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842\n",
      " 0.22326454]\n",
      "[0.13227017 0.42589118 0.34803002 0.09099437 0.13320826 0.24015009\n",
      " 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289 0.20731707\n",
      " 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368 0.09380863\n",
      " 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764 0.19512195\n",
      " 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672 0.34990619\n",
      " 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842 0.22326454\n",
      " 0.33020638]\n",
      "[0.42589118 0.34803002 0.09099437 0.13320826 0.24015009 0.21294559\n",
      " 0.20262664 0.33395872 0.22045028 0.09662289 0.20731707 0.0750469\n",
      " 0.14446529 0.21857411 0.35928705 0.21388368 0.09380863 0.14446529\n",
      " 0.20356473 0.2054409  0.22045028 0.14821764 0.19512195 0.15572233\n",
      " 0.18761726 0.1641651  0.1575985  0.09474672 0.34990619 0.29268293\n",
      " 0.17354597 0.07879925 0.21763602 0.26547842 0.22326454 0.33020638\n",
      " 0.06378987]\n",
      "[0.34803002 0.09099437 0.13320826 0.24015009 0.21294559 0.20262664\n",
      " 0.33395872 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529\n",
      " 0.21857411 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473\n",
      " 0.2054409  0.22045028 0.14821764 0.19512195 0.15572233 0.18761726\n",
      " 0.1641651  0.1575985  0.09474672 0.34990619 0.29268293 0.17354597\n",
      " 0.07879925 0.21763602 0.26547842 0.22326454 0.33020638 0.06378987\n",
      " 0.0891182 ]\n",
      "[0.09099437 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872\n",
      " 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411\n",
      " 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409\n",
      " 0.22045028 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651\n",
      " 0.1575985  0.09474672 0.34990619 0.29268293 0.17354597 0.07879925\n",
      " 0.21763602 0.26547842 0.22326454 0.33020638 0.06378987 0.0891182\n",
      " 0.17260788]\n",
      "[0.13320826 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028\n",
      " 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705\n",
      " 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028\n",
      " 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985\n",
      " 0.09474672 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602\n",
      " 0.26547842 0.22326454 0.33020638 0.06378987 0.0891182  0.17260788\n",
      " 0.16979362]\n",
      "[0.24015009 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289\n",
      " 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368\n",
      " 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764\n",
      " 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672\n",
      " 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842\n",
      " 0.22326454 0.33020638 0.06378987 0.0891182  0.17260788 0.16979362\n",
      " 0.21669794]\n",
      "[0.21294559 0.20262664 0.33395872 0.22045028 0.09662289 0.20731707\n",
      " 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368 0.09380863\n",
      " 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764 0.19512195\n",
      " 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672 0.34990619\n",
      " 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842 0.22326454\n",
      " 0.33020638 0.06378987 0.0891182  0.17260788 0.16979362 0.21669794\n",
      " 0.18667917]\n"
     ]
    }
   ],
   "source": [
    "ds = make_window_dataset(tf_dataset, window_size=37, shift = 1, stride=1)\n",
    "\n",
    "for example in ds.take(10):\n",
    "  print(example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_7_step(batch):\n",
    "  # Shift features and labels one step relative to each other.\n",
    "  return batch[:-7], batch[-7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18949343 0.26172608 0.06660413 0.13227017 0.42589118 0.34803002\n",
      " 0.09099437 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872\n",
      " 0.22045028 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411\n",
      " 0.35928705 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409\n",
      " 0.22045028 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651 ] => [0.1575985  0.09474672 0.34990619 0.29268293 0.17354597 0.07879925\n",
      " 0.21763602]\n",
      "[0.26172608 0.06660413 0.13227017 0.42589118 0.34803002 0.09099437\n",
      " 0.13320826 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028\n",
      " 0.09662289 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705\n",
      " 0.21388368 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028\n",
      " 0.14821764 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985 ] => [0.09474672 0.34990619 0.29268293 0.17354597 0.07879925 0.21763602\n",
      " 0.26547842]\n",
      "[0.06660413 0.13227017 0.42589118 0.34803002 0.09099437 0.13320826\n",
      " 0.24015009 0.21294559 0.20262664 0.33395872 0.22045028 0.09662289\n",
      " 0.20731707 0.0750469  0.14446529 0.21857411 0.35928705 0.21388368\n",
      " 0.09380863 0.14446529 0.20356473 0.2054409  0.22045028 0.14821764\n",
      " 0.19512195 0.15572233 0.18761726 0.1641651  0.1575985  0.09474672] => [0.34990619 0.29268293 0.17354597 0.07879925 0.21763602 0.26547842\n",
      " 0.22326454]\n"
     ]
    }
   ],
   "source": [
    "dense_labels_ds = ds.map(dense_7_step)\n",
    "\n",
    "#create a tensor for holding the labels and features\n",
    "all_inputs = []\n",
    "all_labels = []\n",
    "\n",
    "for inputs,labels in dense_labels_ds.take(3):\n",
    "  print(inputs.numpy(), \"=>\", labels.numpy())\n",
    "\n",
    "for inputs,labels in dense_labels_ds:\n",
    "  all_inputs.append(inputs)\n",
    "  all_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(511, 30), dtype=float64, numpy=\n",
       "array([[0.18949343, 0.26172608, 0.06660413, ..., 0.15572233, 0.18761726,\n",
       "        0.1641651 ],\n",
       "       [0.26172608, 0.06660413, 0.13227017, ..., 0.18761726, 0.1641651 ,\n",
       "        0.1575985 ],\n",
       "       [0.06660413, 0.13227017, 0.42589118, ..., 0.1641651 , 0.1575985 ,\n",
       "        0.09474672],\n",
       "       ...,\n",
       "       [0.43621013, 0.13320826, 0.1782364 , ..., 0.54409006, 0.39868668,\n",
       "        0.12382739],\n",
       "       [0.13320826, 0.1782364 , 0.17917448, ..., 0.39868668, 0.12382739,\n",
       "        0.19418386],\n",
       "       [0.1782364 , 0.17917448, 0.28893058, ..., 0.12382739, 0.19418386,\n",
       "        0.18574109]])>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.concat(all_inputs, axis=0)\n",
    "X = tf.reshape(X, (-1, 30))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(511, 7), dtype=float64, numpy=\n",
       "array([[0.1575985 , 0.09474672, 0.34990619, ..., 0.17354597, 0.07879925,\n",
       "        0.21763602],\n",
       "       [0.09474672, 0.34990619, 0.29268293, ..., 0.07879925, 0.21763602,\n",
       "        0.26547842],\n",
       "       [0.34990619, 0.29268293, 0.17354597, ..., 0.21763602, 0.26547842,\n",
       "        0.22326454],\n",
       "       ...,\n",
       "       [0.19418386, 0.18574109, 0.18574109, ..., 0.57692308, 0.35459662,\n",
       "        0.10225141],\n",
       "       [0.18574109, 0.18574109, 0.24202627, ..., 0.35459662, 0.10225141,\n",
       "        0.16135084],\n",
       "       [0.18574109, 0.24202627, 0.57692308, ..., 0.10225141, 0.16135084,\n",
       "        0.12195122]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = tf.concat(all_labels, axis=0)\n",
    "Y = tf.reshape(Y, (-1, 7))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a forecasting model using the Keras Sequential API\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(256, activation=\"relu\", name=\"layer1\"),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(64, activation=\"relu\", name=\"layer3\"),\n",
    "        layers.Dense(7, name=\"layer4\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0013\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0014\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0011\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.9862e-04\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.6784e-04\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.9391e-04\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.9944e-04\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.8223e-04\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0010\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0010\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.7612e-04\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.3105e-04\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.6270e-04\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.6311e-04\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.1087e-04\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.1060e-04\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.4871e-04\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.0648e-04\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.8043e-04\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.8617e-04\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.4095e-04\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.4757e-04\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.4317e-04\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.8999e-04\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.2814e-04\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.0007e-04\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.2828e-04\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.0191e-04\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.5932e-04\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2628e-04\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.6376e-04\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7142e-04\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.9267e-04\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.1898e-04\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.1788e-04\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.6532e-04\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.0652e-04\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.9455e-04\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.3321e-04\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.2508e-04\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.1375e-04\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.5907e-04\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.4963e-04\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.0024e-04\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.7345e-04\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.5342e-04\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.3526e-04\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2600e-04\n",
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0708e-04\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1382e-04\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0493e-04\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8008e-04\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7836e-04\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4846e-04\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6930e-04\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5868e-04\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6770e-04\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5284e-04\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7630e-04\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3271e-04\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5494e-04\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2488e-04\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7671e-04\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.8859e-04\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7440e-04\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.7203e-04\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.7403e-04\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.4941e-04\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.3744e-04\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5322e-04\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0110e-04\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0004e-04\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7654e-04\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9008e-04\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9836e-04\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9582e-04\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.3301e-04\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.3950e-04\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2835e-04\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5145e-04\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7323e-04\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6280e-04\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5383e-04\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8404e-04\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1404e-04\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9358e-04\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7341e-04\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5979e-04\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0097e-04\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4280e-04\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2752e-04\n",
      "Epoch 102/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.1876e-04\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.1386e-04\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.3371e-04\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.8851e-04\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5337e-04\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4131e-04\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5087e-04\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4944e-04\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2792e-04\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3680e-04\n",
      "Epoch 112/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.3626e-04\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2524e-04\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1819e-04\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3293e-04\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3599e-04\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3927e-04\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6051e-04\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2570e-04\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3591e-04\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6993e-04\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6038e-04\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6207e-04\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.1599e-04\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.8389e-04\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3059e-04\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1223e-04\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9569e-04\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0297e-04\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2112e-04\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2000e-04\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0432e-04\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2521e-04\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2428e-04\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2923e-04\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0871e-04\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5076e-04\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3147e-04\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0998e-04\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.9535e-04\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8852e-04\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9779e-04\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1305e-04\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2389e-04\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7995e-04\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4353e-04\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9900e-04\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.0668e-04\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.0808e-04\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.1088e-04\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6857e-04\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5214e-04\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2030e-04\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9171e-04\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6781e-04\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5287e-04\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4299e-04\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5411e-04\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7266e-04\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8895e-04\n",
      "Epoch 161/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0365e-04\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2301e-04\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.0953e-04\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.9848e-04\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.7234e-04\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1880e-04\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2266e-04\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.3379e-04\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5960e-04\n",
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5007e-04\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1448e-04\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0309e-04\n",
      "Epoch 173/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5311e-04\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5433e-04\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4051e-04\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.3299e-04\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4263e-04\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4504e-04\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3540e-04\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2435e-04\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2141e-04\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0968e-04\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.8581e-05\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.8770e-05\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.8118e-05\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.9343e-05\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.1536e-05\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.2724e-05\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0167e-04\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0281e-04\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1556e-04\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0882e-04\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.9595e-05\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.4691e-05\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.9030e-05\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0383e-04\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3651e-04\n",
      "Epoch 198/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7018e-04\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8446e-04\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0953e-04\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8433e-04\n",
      "Epoch 202/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0633e-04\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.0730e-04\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6601e-04\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5152e-04\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5216e-04\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2905e-04\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.9680e-05\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.8304e-05\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1507e-04\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1923e-04\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.4513e-04\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5521e-04\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4134e-04\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5504e-04\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4634e-04\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6688e-04\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9144e-04\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7676e-04\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2242e-04\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9044e-04\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.0557e-04\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.1391e-04\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.0198e-04\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3228e-04\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0110e-04\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9426e-04\n",
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6607e-04\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5197e-04\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6796e-04\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7605e-04\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6093e-04\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5341e-04\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3635e-04\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2394e-04\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3131e-04\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2734e-04\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3731e-04\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0990e-04\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1947e-04\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1641e-04\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1955e-04\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0262e-04\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.1480e-05\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.0799e-05\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.7503e-05\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.4652e-05\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.7881e-05\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.0262e-04\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.8820e-05\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.3497e-05\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.3480e-05\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.8827e-05\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.0332e-05\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.5669e-05\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.3531e-05\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.0357e-05\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2371e-04\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8168e-04\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5217e-04\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4133e-04\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.0870e-04\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9937e-04\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7754e-04\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8081e-04\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5640e-04\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4463e-04\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3252e-04\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.7873e-05\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0082e-04\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0605e-04\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0578e-04\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1681e-04\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2268e-04\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1601e-04\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.3609e-04\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6299e-04\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7572e-04\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9228e-04\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9594e-04\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0069e-04\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7919e-04\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8767e-04\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7081e-04\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0843e-04\n",
      "Epoch 286/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7925e-04\n",
      "Epoch 287/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2856e-04\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3521e-04\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1185e-04\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0423e-04\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.9544e-05\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.5510e-05\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3502e-04\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6194e-04\n",
      "Epoch 295/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6650e-04\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9836e-04\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9334e-04\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7862e-04\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8110e-04\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6296e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf3c20d5b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=Y ,epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a request for forecasting the next 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Ospiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>2022-11-06</td>\n",
       "      <td>435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>2022-11-07</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>2022-11-09</td>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>2022-11-11</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>2022-11-12</td>\n",
       "      <td>814.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>2022-11-20</td>\n",
       "      <td>542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>2022-11-22</td>\n",
       "      <td>324.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>2022-11-23</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>2022-11-24</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Data  Ospiti\n",
       "517  2022-11-01   489.0\n",
       "518  2022-11-02   221.0\n",
       "519  2022-11-03   285.0\n",
       "520  2022-11-04   371.0\n",
       "521  2022-11-05   701.0\n",
       "522  2022-11-06   435.0\n",
       "523  2022-11-07   206.0\n",
       "524  2022-11-08   294.0\n",
       "525  2022-11-09   349.0\n",
       "526  2022-11-10   304.0\n",
       "527  2022-11-11   419.0\n",
       "528  2022-11-12   814.0\n",
       "529  2022-11-13   431.0\n",
       "530  2022-11-14   194.0\n",
       "531  2022-11-15   284.0\n",
       "532  2022-11-16   322.0\n",
       "533  2022-11-17   306.0\n",
       "534  2022-11-18   469.0\n",
       "535  2022-11-19   697.0\n",
       "536  2022-11-20   542.0\n",
       "537  2022-11-21   249.0\n",
       "538  2022-11-22   324.0\n",
       "539  2022-11-23   315.0\n",
       "540  2022-11-24   315.0\n",
       "541  2022-11-25   375.0\n",
       "542  2022-11-26   732.0\n",
       "543  2022-11-27   495.0\n",
       "544  2022-11-28   226.0\n",
       "545  2022-11-29   289.0\n",
       "546  2022-11-30   247.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y.iloc[-30:]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47580645, 0.04354839, 0.14677419, 0.28548387, 0.81774194,\n",
       "        0.38870968, 0.01935484, 0.16129032, 0.25      , 0.17741935,\n",
       "        0.36290323, 1.        , 0.38225806, 0.        , 0.14516129,\n",
       "        0.20645161, 0.18064516, 0.44354839, 0.81129032, 0.56129032,\n",
       "        0.08870968, 0.20967742, 0.19516129, 0.19516129, 0.29193548,\n",
       "        0.86774194, 0.48548387, 0.0516129 , 0.15322581, 0.08548387]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_scaled = scaler.fit_transform(y_test[\"Ospiti\"].values.reshape(-1,1))\n",
    "y_test_scaled = np.reshape(y_test_scaled, (1, len(y_test_scaled)))\n",
    "y_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-12-01', '2022-12-02', '2022-12-03', '2022-12-04',\n",
       "               '2022-12-05', '2022-12-06', '2022-12-07'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dates to be predicted\n",
    "dates = pd.date_range(start=y[\"Data\"].iloc[-1],periods=8)[1:]\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11520471, 0.45951736, 0.85572845, 0.5390352 , 0.21465878,\n",
       "        0.15482873, 0.20482987]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the next 7 days\n",
    "pred = model.predict(y_test_scaled)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[265.42694, 478.90076, 724.55164, 528.20184, 327.08844, 289.9938 ,\n",
       "        320.9945 ]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_unscaled = scaler.inverse_transform(pred)\n",
    "pred_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ospiti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>265.426941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>478.900757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-03</th>\n",
       "      <td>724.551636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-04</th>\n",
       "      <td>528.201843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>327.088440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>289.993805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>320.994507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Ospiti\n",
       "2022-12-01  265.426941\n",
       "2022-12-02  478.900757\n",
       "2022-12-03  724.551636\n",
       "2022-12-04  528.201843\n",
       "2022-12-05  327.088440\n",
       "2022-12-06  289.993805\n",
       "2022-12-07  320.994507"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct results dataframe\n",
    "results = pd.DataFrame(pred_unscaled[0], columns=[\"Ospiti\"], index=dates)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
